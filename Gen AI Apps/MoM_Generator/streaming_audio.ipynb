{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0711393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fe6569c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:12<00:00, 12.0MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(80, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51865, 512)\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (out): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = whisper.load_model(\"base\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac2e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "# Constants\n",
    "SAMPLE_RATE = 16000\n",
    "BLOCK_SIZE = 4000  # ~0.25s of audio\n",
    "\n",
    "# Load Whisper model (base/small/medium/large)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Queue to hold audio blocks\n",
    "q = queue.Queue()\n",
    "\n",
    "# Audio callback\n",
    "def callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    q.put(indata.copy())\n",
    "\n",
    "# Transcribe audio chunks\n",
    "def transcribe():\n",
    "    print(\"Listening...\")\n",
    "    buffer = np.zeros((0, 1), dtype=np.float32)\n",
    "\n",
    "    while True:\n",
    "        audio_chunk = q.get()\n",
    "        buffer = np.concatenate((buffer, audio_chunk))\n",
    "\n",
    "        # If buffer is larger than 5 seconds, transcribe the latest 5 seconds\n",
    "        if len(buffer) >= SAMPLE_RATE * 5:\n",
    "            segment = buffer[-SAMPLE_RATE*5:]  # Last 5 seconds\n",
    "            audio_np = np.squeeze(segment)\n",
    "\n",
    "            # Run Whisper\n",
    "            result = model.transcribe(audio_np, fp16=False, language='en')\n",
    "            print(\"âž¤\", result['text'].strip())\n",
    "\n",
    "# Start audio stream\n",
    "stream = sd.InputStream(callback=callback, channels=1, samplerate=SAMPLE_RATE, blocksize=BLOCK_SIZE)\n",
    "with stream:\n",
    "    transcribe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a020d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3537"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "756e9740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”Š Total audio length: 740.46 seconds\n",
      "ðŸŽ§ Streaming...\n",
      "ðŸ•’ Chunk 1: A few years ago, I broke into my own house. I had just driven home. It was around midnight in the dead of Montreal winner. I'd been visiting my friend Jeff across town. And the thermometer on the front porch read minus 40 degrees. And don't bother asking if that sells...\n",
      "ðŸ•’ Chunk 2: use her Fahrenheit minus 40 is where the two scales meet. It was very cold. And as I stood on the front porch fumbling in my pockets, I found I didn't have my keys. In fact, I could see them through the window, lying on the dining room table where I had left them. So I quickly ran around and tried all the other doors and windows and they were locked tight. I thought about calling a locksmith, at least I had my cell phone, but at midnight, it could take a while for a locksmith to show up, and it was cold.\n",
      "ðŸ•’ Chunk 3: I couldn't go back to my friend Jeff's house for the night because I had an early flight to Europe the next morning and I needed to get my passport in my suitcase. So desperate and freezing cold, I found a large rock and I broke through the basement window, cleared out the shards of glass, I crawled through, I found a piece of cardboard and taped it up over the whole opening, figuring that in the morning on the way to the airport I could call my contractor and ask him to fix it. This is going to be expensive but probably no more expensive than...\n",
      "ðŸ•’ Chunk 4: in the middle of the night locksmith, so I figured under the circumstances I was coming out even. Now I'm a neuroscientist by training, and I know a little bit about how the brain performs under stress. It releases cortisol, that raises your heart rate, it modulates adrenaline levels, and it clouds your thinking. So the next morning, when I woke up on too little sleep, worrying about the hole in the window and the mental note that I had.\n",
      "ðŸ•’ Chunk 5: call my contractor and the freezing temperatures and the meetings I had up coming in Europe. And you know with all the cortisol in my brain my thinking was cloudy but I didn't know it was cloudy because my thinking was cloudy. And it wasn't until I got to the airport check-in counter that I realized I didn't have my passport. So I raced home in the snow and ice 40 minutes, got my passport, raced back to the airport. I made it just in\n",
      "ðŸ•’ Chunk 6: time, but they had given away my seat to someone else, so I got stuck in the back of the plane next to the bathrooms in a seat that wouldn't recline on an eight-hour flight. Well, I had a lot of time to think during those eight hours it no sleep. And I started wondering, are there things that I can do, systems that I can put into place that will prevent bad things from happening? Or at least if bad things happen, we'll minimize the likelihood of it being a total catastrophe.\n",
      "ðŸ•’ Chunk 7: about that, but my thoughts didn't crystallize until about a month later. I was having dinner with my colleague Danny Coniman, the Nobel Prize winner, and I somewhat embarrassly told him about having broken my window and, you know, forgot my passport. And Danny shared with me that he'd been practicing something called prospective hindsight. It's something that he had gotten from the psychologist Gary Klein, who had written about it a few years before, also called the pre-mortem. Now, you all know what the post-mortem is.\n",
      "ðŸ•’ Chunk 8: whenever there's a disaster, a team of experts come in and they try to figure out what went wrong, right? Well, in the pre-mortem, Danny explained, you look ahead and you try to figure out all the things that could go wrong, and then you try to figure out what you can do to prevent those things from happening or to minimize the damage. So, what I want to talk to you about today are some of the things we can do in the form of a pre-mortem. Some of them are obvious, some of them are not so obvious. I'll start with the obvious ones.\n",
      "ðŸ•’ Chunk 9: around the home, designate a place for things that are easily lost. Now, this sounds like common sense, and it is, but there's a lot of science to back this up, based on the way our spatial memory works. There's a structure in the brain called the hippocampus that evolved over tens of thousands of years to keep track of the locations of important things, where the well is, where fish can be found, that stand of fruit trees.\n",
      "ðŸ•’ Chunk 10: where the friendly and enemy tribes live. The hippocampus is the part of the brain that in London taxi cab drivers becomes enlarged. It's the part of the brain that allows squirrels to find their nuts. And if you're wondering, somebody actually did the experiment where they cut off the olfactory sense of the squirrels and they could still find their nuts. They weren't using smell, they were using the hippocampus. This exquisitely evolved mechanism in the brain for finding things. But it's really good for things that don't move.\n",
      "ðŸ•’ Chunk 11: around much, not so good for things that move around. So this is why we lose car keys and reading glasses and passports. So in the home, designate a spot for your keys. A hook by the door, maybe a decorative bowl, for your passport, a particular drawer, for your reading glasses, a particular table. If you designate a spot and you're scrupulous about it, your things will always be there when you look for them. What about travel? Take a cell phone picture of your credit cards, your driver's license, your passport.\n",
      "ðŸ•’ Chunk 12: mail it to yourself so it's in the cloud. If these things are lost or stolen, you can facilitate replacement. Now these are some rather obvious things. Remember, when you're under stress, the brain releases cortisol, cortisol is toxic, and it causes cloudy thinking. So part of the practice of the pre-mortem is to recognize that under stress, you're not gonna be at your best, and you should put systems in place. And there's perhaps no more stressful a situation than when you're confronted.\n",
      "ðŸ•’ Chunk 13: with a medical decision to make. And at some point, all of us are going to be in that position where we have to make a very important decision about the future of our medical care or that of a loved one to help them with the decision. And so I want to talk about that. And I'm going to talk about a very particular medical condition. But this stands as a proxy for all kinds of medical decision making. And indeed for financial decision making and social decision making, any kind of decision you have to make that would benefit from a rational assessment.\n",
      "ðŸ•’ Chunk 14: of the facts. So, suppose you go to your doctor and the doctor says, I just got your lab work back, your cholesterol is a little high. Now you all know that high cholesterol is associated with an increased risk of cardiovascular disease, heart attacks, stroke. And so you're thinking having high cholesterol isn't the best thing. And so the doctor says, you know, I'd like to give you a drug that will help you lower your cholesterol a statin. And you've probably heard of statins. You know that they're among the most...\n",
      "ðŸ•’ Chunk 15: for scribe drugs in the world today. You probably even know people who take them. And so you're thinking, yeah, give me the statin'. But there's a question you should ask at this point. A statistic you should ask for that most doctors don't like talking about and pharmaceutical companies like talking about even less. It's for the number needed to treat. Now what is this, the NNT? It's the number of people that need to take a drug or undergo a surgery or any medical procedure before one person.\n",
      "ðŸ•’ Chunk 16: in his health. You're thinking what kind of crazy statistic is that? The number should be one. My doctor wouldn't prescribe something to me if it's not going to help, but actually medical practice doesn't work that way. And it's not the doctor's fault. If anybody's fault, it's the fault of scientists, like me, we haven't figured out the underlying mechanisms well enough. But GlaxoSmithCline estimates that 90% of the drugs work in only 30% to 50% of the people. So the number needed to treat for the most widely prescribed statinate.\n",
      "ðŸ•’ Chunk 17: What do you suppose it is? How many people have to take it before one person's helped? 300. This is according to research by research practitioners Jerome Gruppen and Pamela Hartzband, independently confirmed by Bloomberg.com. I ran through the numbers myself. 300 people have to take the drug for a year before one heart attack stroke or other adverse event is prevented. Now you're probably thinking, well, okay, one in 300 chance of lowering my cholesterol. Why not, Doc? Give me the prescription.\n",
      "ðŸ•’ Chunk 18: anyway, but you should ask at this point for another statistic and that is tell me about the side effects, right? So for this particular drug the side effects occur in 5% of the patients and they include terrible things debilitating muscle and joint pain, gastrointestinal distress, but now you're thinking well 5% not very likely it's gonna happen to me, I'll still take the drug, but wait a minute, remember under stress we're not thinking clearly, so think about how you gonna work through this ahead of time so you don't have to manufacture the chain\n",
      "ðŸ•’ Chunk 19: reasoning on the spot. 300 people take the drug, right? One person's helped. 5% of those 300 have side effects. That's 15 people. You're 15 times more likely to be harmed by the drug than you are to be helped by the drug. Now, I'm not saying whether you should take the stat or not. I'm just saying you should have this conversation with your doctor. Medical ethics requires it. It's part of the principle of informed consent. You have the right to have access to this kind of information\n",
      "ðŸ•’ Chunk 20: in the conversation about whether you want to take the risks or not. Now, you might be thinking I've pulled this number out of the air for shock value, but in fact it's rather typical this number needed to treat. For the most widely performed surgery on men over the age of 50, removal of the prostate for cancer, the number needed to treat is 49. That's right, 49 surgeries are done for every one person who's helped. And the side effects in that case occur in 50% of the patients.\n",
      "ðŸ•’ Chunk 21: rectal dysfunction, urinary incontinence, rectal tearing, fecal incontinence, and if you're lucky, and you're one of the 50% who has these the only last for a year or two. So, the idea of the pre-mortem is to think ahead of time to the questions that you might be able to ask that will push the conversation forward. You don't want to have to manufacture all of this on the spot. And you also want to think about things like quality of life, because you have a choice often times, do I want to...\n",
      "ðŸ•’ Chunk 22: shorter life that's pain-free or a longer life that might have a great deal of pain towards the end. These are things to talk about and think about now with your family and your loved ones. You might change your mind in the heat of the moment, but at least you're practiced with this kind of thinking. Remember, our brain under stress releases cortisol. And one of the things that happens at that moment is a whole bunch of systems shut down. There's an evolutionary reason for this. If you face to face with a predator, you don't need your digestive system or you...\n",
      "ðŸ•’ Chunk 23: libido or your immune system because if your body is expending metabolism on those things and you don't react quickly, you might become the lion's lunch. And then none of those things matter. Unfortunately, one of the things that goes out the window during those time to stress is rational logical thinking as Danny Coniman and his colleagues have shown. So we need to train ourselves to think ahead to these kinds of situations. I think the important point here is\n",
      "ðŸ•’ Chunk 24: is recognizing that all of us are flawed. We all are going to fail now and then. The idea is to think ahead to what those failures might be, to put systems in place that will help minimize the damage or to prevent the bad things from happening in the first place. Getting back to that snowy night and Montreal when I got back from my trip, I had my contractor install a combination lock next to the door with a key to the front door in it, an easy-to-reumber combination.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "import time\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Load full MP3 and convert to mono 16kHz\n",
    "audio = AudioSegment.from_file(\"testing_medium.mp3\")\n",
    "audio = audio.set_channels(1).set_frame_rate(16000)\n",
    "\n",
    "chunk_duration_ms = 30000  # 5 seconds\n",
    "num_chunks = len(audio) // chunk_duration_ms\n",
    "\n",
    "print(f\"ðŸ”Š Total audio length: {len(audio)/1000:.2f} seconds\")\n",
    "print(\"ðŸŽ§ Streaming...\")\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    chunk = audio[i * chunk_duration_ms: (i + 1) * chunk_duration_ms]\n",
    "    \n",
    "    # Convert to numpy float32 array\n",
    "    samples = np.array(chunk.get_array_of_samples()).astype(np.float32) / 32768.0\n",
    "\n",
    "    # Transcribe using Whisper\n",
    "    result = model.transcribe(samples, fp16=False, language=\"en\")\n",
    "    print(f\"ðŸ•’ Chunk {i+1}: {result['text'].strip()}\")\n",
    "\n",
    "    time.sleep(5)  # simulate real-time delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "401dca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Transcript:  If you want to change the world, start off by making your bed. If you make your bed every morning, you will have accomplished the first task of the day. It will give you a small, since a pride, and it will encourage you to do another task, and another, and another. And by the end of the day, that one task completed will have turned into many tasks completed. And your bed will also reinforce the fact that the little things in life matter. If you can't do the little things right, you'll never be able to do the big things right. And if by chance you have a miserable day, you will come home to a bed that is made. That you made, and a made bed gives you encouragement that tomorrow will be better. I've been a Navy SEAL for 36 years. Every morning in SEAL training, my instructors, who at the time were all Vietnam veterans, would show up in my barritch room, and the first thing they'd do was inspect my bed. If you did it right, the corners would be square, the covers would be pulled tight, the pillow centered just under the headboard, and the extra blanket folded neatly at the foot of the rack. It was a simple task, mundane at best. But every morning, we were required to make our bed a perfection. It seemed a little ridiculous at the time, particularly in light of the fact that we were aspiring to be real warriors, tough battle-hardened SEALs. But the wisdom of this simple act has been proven to me many times over. It matters not whether you ever served a day in uniform. It matters not your gender, your ethnic or religious background, your orientation, or your social status. Our struggles in this world are similar, and the lessons to overcome those struggles, and to move forward, changing ourselves and changing the world around us, will apply equally to all. If you think it's hard to change the lives of 10 people, change their lives forever, you're wrong. I saw it happen every day in Iraq and Afghanistan. But changing the world can happen anywhere and anyone can do it. So what starts here can indeed change the world. You will likely fail often, and it will be painful. It will be discouraging. At times, it will test you to your very core. At that darkest moment of the mission is a time when you need to be calm. When you must be calm, when you must be composed. When all your tactical skills, your physical power, and your inner strength must be brought to bear, if you want to change the world, you must be your very best in the darkest moments. If I have learned anything in my time traveling the world, it is the power of hope, the power of one person, a Washington, a Lincoln, King Mandela, and even a young girl from Pakistan, Malaala. One person can change the world by giving people hope. Start each day with a task completed. Find someone to help you through life, respect everyone. Know that life is not fair and that you will fail often. But if you take some risks, step up when the times are the toughest. Face down the bullies, lift up the downtrodden, and never ever give up. If you do these things, the next generation and the generations that follow will live in a world far better than the one we have today. And what started here will indeed have changed the world for the better. Finally in Seal Training there is a bell. A brass bell that hangs in the center of the compound for all the students to see. All you have to do to quit is ring the bell. Ring the bell and you no longer have to wake up at 5 o'clock. Ring the bell and you no longer have to be in the freezing cold swims. Ring the bell and you no longer have to do the runs, the obstacle course, the PT. And you no longer have to endure the hardships of training. All you have to do is ring the bell to get out. If you want to change the world, don't ever, ever ring the bell.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Pass file path\n",
    "result = model.transcribe(\"testing.mp3\", language=\"en\")\n",
    "print(\"ðŸ“ Transcript:\", result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ed54e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Pass file path\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtesting_medium.mp3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ“ Transcript:\u001b[39m\u001b[33m\"\u001b[39m, result[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:295\u001b[39m, in \u001b[36mtranscribe\u001b[39m\u001b[34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    293\u001b[39m     decode_options[\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m] = all_tokens[prompt_reset_since:]\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m result: DecodingResult = \u001b[43mdecode_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_segment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m tokens = torch.tensor(result.tokens)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_speech_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# no voice activity check\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:201\u001b[39m, in \u001b[36mtranscribe.<locals>.decode_with_fallback\u001b[39m\u001b[34m(segment)\u001b[39m\n\u001b[32m    198\u001b[39m     kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mbest_of\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    200\u001b[39m options = DecodingOptions(**kwargs, temperature=t)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m decode_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m needs_fallback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    205\u001b[39m     compression_ratio_threshold \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    206\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m decode_result.compression_ratio > compression_ratio_threshold\n\u001b[32m    207\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\decoding.py:824\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(model, mel, options, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m    822\u001b[39m     options = replace(options, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m result = \u001b[43mDecodingTask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m single \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\decoding.py:737\u001b[39m, in \u001b[36mDecodingTask.run\u001b[39m\u001b[34m(self, mel)\u001b[39m\n\u001b[32m    734\u001b[39m tokens = tokens.repeat_interleave(\u001b[38;5;28mself\u001b[39m.n_group, dim=\u001b[32m0\u001b[39m).to(audio_features.device)\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# call the main sampling loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m tokens, sum_logprobs, no_speech_probs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_main_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# reshape the tensors to have (n_audio, n_group) as the first two dimensions\u001b[39;00m\n\u001b[32m    740\u001b[39m audio_features = audio_features[:: \u001b[38;5;28mself\u001b[39m.n_group]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\decoding.py:687\u001b[39m, in \u001b[36mDecodingTask._main_loop\u001b[39m\u001b[34m(self, audio_features, tokens)\u001b[39m\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.sample_len):\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    690\u001b[39m             i == \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenizer.no_speech \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    691\u001b[39m         ):  \u001b[38;5;66;03m# save no_speech_probs\u001b[39;00m\n\u001b[32m    692\u001b[39m             probs_at_sot = logits[:, \u001b[38;5;28mself\u001b[39m.sot_index].float().softmax(dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\decoding.py:163\u001b[39m, in \u001b[36mPyTorchInference.logits\u001b[39m\u001b[34m(self, tokens, audio_features)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokens.shape[-\u001b[32m1\u001b[39m] > \u001b[38;5;28mself\u001b[39m.initial_token_length:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# only need to use the last token except in the first forward pass\u001b[39;00m\n\u001b[32m    161\u001b[39m     tokens = tokens[:, -\u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\model.py:242\u001b[39m, in \u001b[36mTextDecoder.forward\u001b[39m\u001b[34m(self, x, xa, kv_cache)\u001b[39m\n\u001b[32m    239\u001b[39m x = x.to(xa.dtype)\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln(x)\n\u001b[32m    245\u001b[39m logits = (\n\u001b[32m    246\u001b[39m     x @ torch.transpose(\u001b[38;5;28mself\u001b[39m.token_embedding.weight.to(x.dtype), \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    247\u001b[39m ).float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\model.py:167\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    161\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    162\u001b[39m     x: Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    165\u001b[39m     kv_cache: Optional[\u001b[38;5;28mdict\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    166\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn_ln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cross_attn:\n\u001b[32m    169\u001b[39m         x = x + \u001b[38;5;28mself\u001b[39m.cross_attn(\u001b[38;5;28mself\u001b[39m.cross_attn_ln(x), xa, kv_cache=kv_cache)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\model.py:105\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, xa, mask, kv_cache)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m xa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kv_cache:\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;66;03m# otherwise, perform key/value projections for self- or cross-attention as usual.\u001b[39;00m\n\u001b[32m    104\u001b[39m     k = \u001b[38;5;28mself\u001b[39m.key(x \u001b[38;5;28;01mif\u001b[39;00m xa \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xa)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     v = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# for cross-attention, calculate keys and values once and reuse in subsequent calls.\u001b[39;00m\n\u001b[32m    108\u001b[39m     k = kv_cache[\u001b[38;5;28mself\u001b[39m.key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1845\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1842\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m   1844\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1849\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[32m   1850\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1793\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1790\u001b[39m     bw_hook = BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[32m   1791\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n\u001b[32m   1795\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1796\u001b[39m         *_global_forward_hooks.items(),\n\u001b[32m   1797\u001b[39m         *\u001b[38;5;28mself\u001b[39m._forward_hooks.items(),\n\u001b[32m   1798\u001b[39m     ):\n\u001b[32m   1799\u001b[39m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\model.py:48\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.linear(\n\u001b[32m     47\u001b[39m         x,\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     49\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias.to(x.dtype),\n\u001b[32m     50\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "168fb542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6135"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "If you want to change the world, start off by making your bed. If you make your bed every morning, you will have accomplished the first task of the day. It will give you a small, since a pride, and it will encourage you to do another task, and another, and another. And by the end of the day, that one task completed will have turned into many tasks completed.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(\"testing.mp3\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "591a7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "A few years ago, I broke into my own house. I had just driven home. It was around midnight in the dead of Montreal winner. I'd been visiting my friend Jeff across town. And the thermometer\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(\"testing_medium.mp3\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34b73b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "Change the world. It's 24. It's an honor to present to you the one and only, the goat, the legend, Graham Weaver.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "audio = whisper.load_audio(\"testing_large.mp3\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb5056f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32106324"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio = whisper.load_audio(\"testing_large.mp3\")\n",
    "len(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d60efcf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "incorrect audio shape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# detect the spoken language\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m _, probs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetected language: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(probs,\u001b[38;5;250m \u001b[39mkey=probs.get)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# decode the audio\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\decoding.py:52\u001b[39m, in \u001b[36mdetect_language\u001b[39m\u001b[34m(model, mel, tokenizer)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# skip encoder forward pass if already-encoded audio features were given\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mel.shape[-\u001b[32m2\u001b[39m:] != (model.dims.n_audio_ctx, model.dims.n_audio_state):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     mel = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# forward pass using a single token, startoftranscript\u001b[39;00m\n\u001b[32m     55\u001b[39m n_audio = mel.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\sharath\\Github\\Projects\\Loan prediction\\loan-pred\\.venv\\Lib\\site-packages\\whisper\\model.py:197\u001b[39m, in \u001b[36mAudioEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    194\u001b[39m x = F.gelu(\u001b[38;5;28mself\u001b[39m.conv2(x))\n\u001b[32m    195\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m x.shape[\u001b[32m1\u001b[39m:] == \u001b[38;5;28mself\u001b[39m.positional_embedding.shape, \u001b[33m\"\u001b[39m\u001b[33mincorrect audio shape\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m x = (x + \u001b[38;5;28mself\u001b[39m.positional_embedding).to(x.dtype)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n",
      "\u001b[31mAssertionError\u001b[39m: incorrect audio shape"
     ]
    }
   ],
   "source": [
    "mel = whisper.log_mel_spectrogram(audio, n_mels=model.dims.n_mels).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions()\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2192210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88219fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Change the world. It's 24. It's an honor to present to you the one and only, the goat, the legend, Graham Weaver.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb35116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
