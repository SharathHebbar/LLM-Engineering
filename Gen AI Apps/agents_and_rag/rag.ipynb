{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "407efd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b685742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "model_name = \"medgemma-4b-it\"\n",
    "emb_model_name = \"nomic-embed-text-v1.5\"\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    model=model_name,\n",
    "    api_key=\"lm-studio\"\n",
    ")\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     base_url=\"http://localhost:1234/v1\",\n",
    "#     model=emb_model_name,\n",
    "#     api_key=\"lm-studio\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560d876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import requests\n",
    "from langchain.embeddings.base import Embeddings\n",
    "class NomicEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name:str, base_url:str=\"http://localhost:1234/v1\", api_key:str=\"lm-studio\"):\n",
    "        self.model_name = model_name\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self.embed_query(text) for text in texts]\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        url = f\"{self.base_url}/embeddings\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-type\": \"application/json\"\n",
    "        }\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"input\": text\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['data'][0]['embedding']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98038c4",
   "metadata": {},
   "source": [
    "# Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Langchain used for?\"\n",
    "print(llm.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4860bab",
   "metadata": {},
   "source": [
    "# Add External Knowledge with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2729a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LangChain is a framework for developing applications powered by large language models (LLMs).',\n",
       " 'LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).',\n",
       " 'LangChain provides a structured way to connect LLMs with external data sources, enabling more powerful and context-aware applications like chatbots and virtual assistants',\n",
       " 'LangChain acts as a bridge, allowing developers to combine the capabilities of LLMs with other tools and data to create more complex and intelligent systems.',\n",
       " 'LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [\n",
    "    \"LangChain is a framework for developing applications powered by large language models (LLMs).\",\n",
    "    \"LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).\",\n",
    "    \"LangChain provides a structured way to connect LLMs with external data sources, enabling more powerful and context-aware applications like chatbots and virtual assistants\",\n",
    "    \"LangChain acts as a bridge, allowing developers to combine the capabilities of LLMs with other tools and data to create more complex and intelligent systems.\",\n",
    "    \"LangChain implements a standard interface for large language models and related technologies, such as embedding models and vector stores, and integrates with hundreds of providers.\",\n",
    "]\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c441080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x28923120830>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = NomicEmbeddings(model_name=\"nomic-embed-text-v1.5\")\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e9db59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'NomicEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028923120830>, search_kwargs={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eafce6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(verbose=False, combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000288DABC1760>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000288DAC8B980>, root_client=<openai.OpenAI object at 0x00000288B95D0530>, root_async_client=<openai.AsyncOpenAI object at 0x00000288D96A5580>, model_name='medgemma-4b-it', model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='http://localhost:1234/v1'), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), retriever=VectorStoreRetriever(tags=['FAISS', 'NomicEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000028923120830>, search_kwargs={}))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c32b8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is used for developing applications powered by large language models (LLMs). It provides a structured way to connect LLMs with external data sources, enabling more powerful and context-aware applications like chatbots and virtual assistants. LangChain acts as a bridge, allowing developers to combine the capabilities of LLMs with other tools and data to create more complex and intelligent systems.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.run(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b8cb70",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64142523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain supports document Q&A by providing a structured way to connect LLMs with external data sources, enabling more powerful and context-aware applications like chatbots and virtual assistants.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "\n",
    "response = qa.invoke(\"How does Langchain support document QnA\")\n",
    "print(response['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8112d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LangChain provides a structured way to connect LLMs with external data sources, enabling more powerful and context-aware applications like chatbots and virtual assistants\n",
      "- LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).\n"
     ]
    }
   ],
   "source": [
    "for doc in response['source_documents']:\n",
    "    print(\"-\", doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be114fc",
   "metadata": {},
   "source": [
    "# Common RAG Challenges\n",
    "\n",
    "1. Poor Chunking Strategy: Split documents intelligently - use headers, sections or semantic cues.\n",
    "2. Low quality embeddings: Clean input leads to better vector quality.\n",
    "3. Retrieval depth (k too low): Try k=3 or k=5 for better context coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d447f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
